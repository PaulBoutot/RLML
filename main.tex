\documentclass[11pt,letterpaper]{ryersonSGSThesis}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\newcommand{\heading}[1]{\bfseries #1}
\renewcommand{\arraystretch}{1.3}
\interdisplaylinepenalty=2500
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\algorithmCaption}[1]{
\hrule height 1pt
\vspace*{\jot}
\textbf{Algorithm:} #1
\vspace*{\jot}
\hrule
}

\DeclareMathOperator*{\argmax}{arg\ max}
\algnewcommand\algorithmicdefn{\textbf{Definitions:}}
\algnewcommand\Defn{\item[\algorithmicdefn]}
\UseRawInputEncoding
\usepackage[utf8]{inputenc}
\usepackage{url}


\usepackage{listings,lstautogobble}
\usepackage{xcolor}
\usepackage[section]{placeins}

\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}
\lstset{language=Java,
    keywordstyle=\color{RoyalBlue},
    basicstyle=\scriptsize\ttfamily,
    commentstyle=\ttfamily\itshape\color{gray},
    stringstyle=\ttfamily,
    showstringspaces=false,
    breaklines=true,
    frameround=ffff,
    frame=single,
    rulecolor=\color{black},
    autogobble=true
}

\setAuthor{Natalie Sinani}
\setTitle{{RLML: A Domain-Specific Modelling Language for Reinforcement Learning}}
\setThesisDegree{Master of Science (MSc)}
\setPastDegreeA{Bachelors, Aleppo University of Syria, 2014}
\setUniversity{Ryerson University}
\setDepartment{Computer Science}
\setLocation{Toronto, Ontario, Canada}
\setThesisYear{2022}
\setAuthorsDeclaration{ I hereby declare that I am the sole author of this MRP. This is a true copy of the MRP,
including any required final revisions.

I authorize Ryerson University to lend this MRP to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this MRP by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my MRP may be made electronically available to the public.
}
\setAbstract{
In recent years, machine learning technologies have gained intense popularity and are being used in a wide range of domains. However, due to the complexity associated with machine learning algorithms, it is a challenge to make it user-friendly, easy to understand and implement. Machine learning applications are especially challenging for users who do not have proficiency in this area. In this work, we use model-driven engineering (MDE) methods and tools for developing a domain-specific modelling language (DSML) to contribute towards providing a solution for this problem. We targeted reinforcement learning domain from machine learning technologies, and evaluated the proposed language with multiple applications. We built a domain-specific modelling environment to support our reinforcement learning modelling language (RLML). The tool supports syntax-directed editing, constraint checking, and automatic generation of code from RLML models. With our proposed approach, we were able to move away from the complexity of implementing machine learning algorithms in general purpose languages and offer abstraction and simplicity for non-experts, which are a few of the characteristics and benefits of modelling languages.
}
\subfile{rsgs-glossary}

\begin{document}
\begin{ryersonSGSThesis}

\chapter{Introduction}
\label{chap:Introduction}
    Model-Driven Engineering (MDE) \cite{Brambilla2017} is a software engineering methodology that uses software models as first class citizens. The heart and soul of MDE are model transformations, which enable automation in MDE. In MDE, domain-specific modelling (DSM) focuses on creating domain models that define a system domain at a high level of abstraction. Thanks to the abstraction that the domain-specific modelling approach provides, it offers a better understanding of complex systems. MDE also provides methods and tools for analysis, simulation and code generation. These features make MDE a good solution whenever abstraction and automation are necessary. 
    
    Recently, artificial intelligence and machine learning technologies are rapidly proliferating around the world. Innovative developments keep emerging, from deepfake videos that blur the line between reality and computer produced videos, to highly advanced algorithms capable of driving cars in intricate and tricky environments with little to no human involvement. Hence, there is an increasing need to make machine learning algorithms more adaptable, intuitive and user friendly. Reinforcement learning \cite{Brunton2019} is one of the three basic machine learning paradigms alongside supervised learning and unsupervised learning. It can be viewed as a form of supervised learning as it is usually given partial information. Reinforcement learning was initially mainly used for gaming, and through its means, the machine was able to compete and win against the most professional players, like chess players, but lately it is increasing in popularity, because it is suited for problems with dynamic and adaptive environments. It is a powerful tool for training AI models that can help with various tasks, like robot learning and environment exploration. It can be used in health care, stock trading and much more. However the challenge is, like it is the case with most machine learning algorithms and applications, that it is quite complex to understand, follow and implement, as it executes complex calculations, therefore it requires proficiency in the domain, before being able to use the technology, and hence, it is reserved for the domain experts. Most of the research work for the machine learning domain is towards contribution in enhancing algorithms and approaches, and in achieving better accuracy and results in prediction and learning, but there is not much work done in the area of simplifying the implementation and application of these complex algorithms. On the other hand, domain-specific modelling languages attempt to do exactly what we are missing from simplicity and abstraction. MDE can contribute in the goal of fair and explainable AI by providing a tool that can directly express and manipulate domain-specific problems, and additionally it provides transformation pipelines to produce lower level general purpose languages to easily run the solution anywhere.
 
    \section{Contribution}
        To contribute in simplifying and abstracting machine learning applications, we are proposing a domain-specific modelling language for the reinforcement learning domain, which we named Reinforcement Learning Modelling Language (RLML). Our contribution is motivated by "Grand challenges in model-driven engineering" paper \cite{Bucchiarone2020}. This paper reviews the challenges that exist in model-driven engineering (MDE) and summarizes the grand challenges facing the MDE community. MDE already made incredible contributions in various software and systems development area, by leveraging automation and abstraction, and we can also commence that in machine learning. The "Grand challenges in model-driven engineering" paper \cite{Bucchiarone2020} describes the benefits that MDE can offer to artificial intelligence and machine learning domain, which is exactly what our research tries to achieve. Machine learning is assuredly a profitable science, but it comes with a number of challenges. Some of these challenges are technical and some are non-technical. The non-technical challenges are described in the paper \cite{Baier2019} and is briefly summarized here:
        
        \begin{itemize}
            \item Difficulty to communicate the machine learning approach to non-machine learning expert customers. 
            \item Challenge due to non-standardization and non-user friendliness of machine learning applications.
            \item A need for convenient machine learning application tools in order to enable non-technical employees to apply ML models.
            \item Often experts with different levels and different technical backgrounds work together on a project and it is hard to have them on the same level of understanding without a tool that simplifies ML results and applications.
        \end{itemize}
            
        The proposed domain-specific language is a contribution to address the non-technical challenges often faced in ML applications. The fundamental advantages of DSLs are \cite{books/daglib/0030751}:
        
        \begin{itemize}
            \item Domain-specific languages are more expressive than general programming languages. Often the resulting program is shorter and more compact compared to the one written in general purpose languages. The semantics are more simple and understandable since it represents the domain in high level, and it encapsulates the technical knowledge in the language and execution strategy.
            \item DSLs provide separation of concerns. A domain can be broken down into different concerns, and each concern is responsible for a specific area of the overall domain. All the aspects of the domain needs to be addressed. This facilitates and simplifies the language development and it disentangles the complexity of the target domain.
            \item The obvious advantage of DSLs is the level of abstraction it offers. Usually domain experts, validate, understand and simplify the domain and write a convenient and user friendly language for all levels of expertise.
        \end{itemize}

        Through the coupling of the two technologies, reinforcement learning and domain-specific modelling, our goal is to abstract the reinforcement learning application and help facilitate the understanding and use of reinforcement learning algorithms by non-experts in machine learning domain to solve various problems. In the proposed language, we try to address the limitations and challenges described in the machine learning domain with the advantages of model-driven domain-specific languages. 

    \section{Report Organization}
        The rest of the report is organized as follows:
        \begin{enumerate}
          \item In Chapter 2, we provide a background for both of the technologies used in this research; model-driven engineering and machine learning with a focus on reinforcement learning.
          \item In Chapter 3, we propose our domain-specific modelling language, RLML, and discuss the abstract syntax, concrete syntax and constraints of RLML.
          \item In Chapter 4, we discuss the domain-specific modelling environment using the language workbench, MPS JetBrains. We focus on the MPS editor and the code generation aspects, and how we used it in our research.
          \item In Chapter 5, we give a detailed description of the reinforcement learning applications we used to validate our proposed reinforcement learning modelling language.
          \item In Chapter 6, we go over the related work to our research, and in Chapter 7, we present the conclusion, challenges and future work.
        \end{enumerate}

\chapter{Background}
\label{chap:background}
     Since we collaborate two technologies in this research, model-driven engineering and machine learning, we will provide a brief background on both technologies in this chapter. We focus on reinforcement learning from machine learning domain, and domain-specific languages from model-driven engineering domain.
     
    \section{Model-Driven Engineering}
        In model-driven engineering, models are first class citizens. Model-driven engineering allows creating domain models which are conceptual models of all the details related to a specific domain \cite{Schmidt2006}. The domain-specific models cover abstract representations of the knowledge that governs a particular application domain, rather than the computing or algorithmic concepts. As a result, MDE allows users to build complex applications through abstraction and reduces human-process interaction through automation.
        In addition to these two key benefits, abstraction and automation, model-driven engineering supports reuse, by allowing its users to add new components, extend existing models, and transform models to different models, by developing model-to-model transformations. MDE aims to increase productivity by simplifying the process of design, and promoting communication between individuals and teams working on the system. One type of domain-driven engineering is defining and using domain-specific languages (DSL).

        \subsection{Domain-Specific Languages}
            Domain-specific languages specialize in a particular domain, which is the contrast of general purpose languages (GPL), which is broadly applicable to any domain \cite{Brambilla2017}. DSLs can be further subdivided into domain-specific markup languages, domain-specific modelling languages, and domain-specific programming languages. Our work proposes a domain-specific modelling language (DSML) we will refer it as DSL for the rest of this report for simplicity.
            Domain-specific modelling languages are usually thought to be graphical languages, however, that is not always the case. Modelling languages can be graphical languages, or textual languages \cite{Brambilla2017}. Using DSLs results in a multitude of benefits, such as the following:
            
            \begin{itemize}
                \item Increases productivity, as the user of the DSL does not have to worry about the low level computational and algorithmic technical details.
                \item Enables domain related validation and verification, because DSLs are more expressive of the target domain than GPLs.
                \item Domain-specific languages exhibit minimal redundancy.
            \end{itemize}
            
            To develop a DSL, we need to define the abstract syntax (otherwise referred to as a metamodel), concrete syntax, and the semantics of the language. Metamodel is a model of a model, and metamodelling is the process of creating a metamodel, and the language for defining a modelling language is called meta-metamodel.
            Abstract syntax defines the essential concepts and structures of the modelling language, where concrete syntax defines the notation or the visual appearance for the concepts defined by the abstract syntax, and how these abstract concepts are realized in a concrete notation \cite{Kahlaoui2008}. The concrete syntax can be one of the two types: Graphical Concrete Syntaxes (GCS) or Textual Concrete Syntaxes (TCS) \cite{Brambilla2017}. To create models conforming to a given DSL, a domain-specific modelling environment (DSME) (also referred to as a modelling editor) needs to be developed. There are various available tools, both commercial and research prototypes, that we can use to build DSMEs. These MDE tools are referred to as language workbenches. By developing a DSL, we build a language that processes and represents important aspects of the domain, and can produce solutions for the selected domain. 
            
        \subsection{Model Transformations}
            One of the greatest advantages of model-driven software development
            is the ability to map models from one form to another. The goal of model transformations is to save effort and reduce errors by automatically producing a model suitable for a targeted purpose. Model transformation can happen through multiple steps. At each step we have a source model and a target model, where the model transformation engineer needs to declare rules for mapping the models from the source to the target model. Model transformations can be of two types: model-to-model (M2M) and model-to-text (M2T), includes model-to-code).
    
    Models of a DSL can be transformed to other models (using M2M, transformations) or to code (using M2T transformations). The transformed models must conform to a target modelling language (M2M) or a programming language (M2T), such as Java, Python, C. DSMEs can be built to provide support for M2M or M2T transformations. Various language workbenches supports automated code generation feature.
    
    \section{Machine Learning}
        Machine learning \cite{Jiang2021} (ML) is seen as part of artificial intelligence (AI) (see Fig. \ref{fig:AI}). The study of machine learning is the study of the automatic learning algorithms that emerged as subfield of AI. Machine learning algorithms focuses on automatically exploiting the training data to build mathematical models to learn data patterns and produce accurate predictions \cite{Jiang2021}. Machine learning allows a user to feed the computer with immense amount of data and let the computer process, analyze and make data driven recommendations and decisions.
        
        \begin{figure}[!tbh]
            \centering
            \includegraphics[width=8cm]{images/ML/AI.png}
            \caption{The Major Goals of Artificial Intelligence (Reproduced from \cite{Nian2020})}
            \label{fig:AI}
        \end{figure}
        
        Primarily machine learning helps in uncertain situations for predictions or in computationally complex situations to learn and reveal data patterns. It is an exciting domain because it is allowing solutions that were not possible before the machine learning era. 
        
        \subsection{Reinforcement Learning} 
            Reinforcement learning is an area of machine learning (see Fig. \ref{fig:MLRL}), concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of rewards (defined later in this section). It is an independent, self teaching system, trying to find an appropriate action model that would maximize an agent’s total cumulative reward, by following trial and error concept. In general, the RL algorithms reward the agent for taking the desired actions in the environment, and punishes i.e., grants negative or zero rewards, for the undesired ones \cite{Brunton2019}. 
    
            \begin{figure}[!tbh]
                \centering
                \includegraphics[width=7cm]{images/ML/MLAndRL.png}
                \caption{Sub-components of Machine Learning (Reproduced from \cite{Nian2020})}
                \label{fig:MLRL}
            \end{figure}
            
            \begin{figure}[!tbh]
                \centering
                \includegraphics[width=7cm]{images/ML/ReinforcementLearning.jpeg}
                \caption{The Agent-Environment Interaction in Reinforcement Learning (Reproduced from \cite{Montague1999})}
                \label{fig:RLElements} 
            \end{figure}
            
            A typical setting where reinforcement learning operates is shown in Figure \ref{fig:RLElements}. The following are the key components that describe reinforcement learning problems:
            
            \begin{itemize}
                \item Environment: reinforcement learning environment \cite{Graesser2019}, represents all the existing states, that the agent can enter. It produces information that describe the states of the system. The agent interacts with the environment by observing the state information and taking an action based on the observation. The environment is everything outside the agent.
                \item State: the state represents the current situation of the agent.
                \item Reward: the environment feeds the agent with rewards. Rewards are numerical values that the agent tries to maximize over time.
                \item Action: the mechanism by which the agent transitions between states of the environment.
                \item Agent: the agent is represented by an intelligent reinforcement learning algorithm that learns and makes decisions to maximize the future rewards while moving within the environment. 
            \end{itemize}
            
        \subsection{Reinforcement Learning Algorithms}
        \label{subsection:Reinforcement Learning Algorithms}
            Reinforcement learning algorithms estimate how \emph{good} it is for the agent to be in a certain state, this estimation is the calculation of what is known as value function \cite{Montague1999}. Value function gets measured based on the expected future rewards that the agent will receive starting from a given state \emph{s}, and according to the actions that the agent will make, and this is called in reinforcement learning as the expected return.
            The goal of a reinforcement learning algorithm is to find the optimal policy for an agent to follow that maximizes the expected return. An optimal policy will have the highest possible value in every state. The optimal policy is implicit and can be derived directly from the optimal value function. There are many different approaches to find the optimal policy. They are mainly categorized as model-based or model-free learning algorithms, and additionally there is deep reinforcement learning (refer to Fig. \ref{fig:RLCats}). It is worth mentioning that this categorization is not comprehensive, and it is often blurry \cite{Brunton2019}.
            
            When the model of the environment is known, which is the case with model-based algorithms, the reinforcement learning problem is simpler and we can utilize policy iteration or value iteration algorithms, to learn the optimal policy or value function, in this approach we either have an access to the model (environment) and we know the probability distribution over states, or we first try to build a model. When the agent knows its model and probability distribution over states it can use it to plan its next moves. However, it is more challenging when we are dealing with model-free algorithms, and it is often the case in real life scenarios, where the agent does not know the environment and needs to discover it.
            As stated by Sutton and Barto "Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning" \cite{Montague1999}. And finally, deep reinforcement learning incorporates deep learning techniques and algorithms in order to learn the model \cite{silver2015}.  
            
            \begin{figure}[!tbh]
                \centering
                \includegraphics[width=12cm]{images/ML/RLCategories.png}
                \caption{RL Categories - reproduced from \cite{Brunton2019}}
                \label{fig:RLCats}
            \end{figure}
           
        \subsection{Model-free Reinforcement Learning}
            In the model-free approaches, the model is not given, hence the agent learns and evaluates how good actions are, by trial and error concept. The agent relies on the past experiences to derive the optimal policy (described earlier). There are also in this approach various available algorithms (see Fig. \ref{fig:RLCats}), to name a few:
            
            \begin{itemize}
                \item Monte Carlo learning
                \item Temporal difference (TD) learning.
                \item SARSA: State–action–reward–state–action learning
                \item Q-Learning
                \item Policy Gradient Optimization
            \end{itemize}
            
            The objective in model-free gradient-free algorithms is to maximize an arbitrary score, which is the value function, these algorithms are also known as value-based algorithms. In value-based methods, the algorithm does not store any explicit policy, only a value function.
            Q-Learning, perhaps, is one of the most dominant model-free algorithm which learns the Q-function directly from experience, without requiring access to a model.
            
            As we see, there are various reinforcement learning algorithms, and in this section we tried to present a brief overview of it. It is important to mention that the implemented reinforcement learning algorithms for this project falls under model-free gradient-free algorithms.
    
    Since we are using both model-driven engineering and machine learning technologies in our research, we would like to clarify that models have different meaning in each of these two technologies. Models in MDE are abstract representation of the elements that define the studied system domain \cite{Schmidt2006}. On the other hand, models in machine learning are algorithms that contain defined instruction and mathematical formulations \cite{Jiang2021}. Models in ML can be trained to recognize certain patterns in provided data.
    
\chapter{RLML - Reinforcement Learning Modelling Language}
\label{chap:RLML}
    The core language concepts of the proposed domain-specific modelling language is inspired and designed from the main elements which represent the reinforcement learning problem and solution algorithm. It can be summarized to; the environment representing the reinforcement learning problem, and the agent which is learning and exploring in the environment.
    
    To reflect the above reinforcement learning elements, the RLML mainly consists of environment element and agent element. Successively these elements contain all the other details involved in solving an RL problem.
        
    \section {RLML Metamodel}
        As mentioned above, the RLML metamodel has two main elements, the environment and the agent, and additionally, the result element. Fig. \ref{fig:RLMLMetamodel} represents the metamodel for RLML.
        
        \begin{figure}[!tbh]
            \centering
            \includegraphics[width=13cm]{images/RLML.png}
            \caption{RLML Metamodel}
            \label{fig:RLMLMetamodel}
        \end{figure}
        
        \textbf{RLML:} The RLML element is the parent element of all the other elements in the language, (referred to as root in MPS), it contains the environment element, reinforcement learning agent and the result. It also has a string property which represents the name of the project.
        
        \textbf{Environment:} The environment element represents the reinforcement learning problem environment, which is used to describe the RL problem and the goal that the agent will learn. It is further broken down into states, actions, terminal states and rewards elements. Each one of these elements contains a value property. These value properties expect a string value and have associated constraints.
        
        \textbf{Reinforcement Learning Agent:} The RL agent in the reinforcement learning domain is represented by the RL algorithm, which will be used to to solve the reinforcement learning problem, given by the RL environment. Therefore, the agent contains a child element which references to the chosen reinforcement learning algorithm fit to solve the problem.
        
        \textbf{Reinforcement Learning Algorithm:} This is the child element of RL agent and it is an \emph{abstract} element. Reinforcement learning algorithm is the parent element for the many different RL algorithms which can be chosen and implemented to solve the RL problem. It holds settings property which references to the required settings and parameters to tune an RL algorithm. All the child RL algorithms will inherit the settings property from the RL agent parent element. The settings element carry the common reinforcement learning algorithm parameters. In addition to the common parameters, a specific type of RL algorithm can carry its own can particular properties.
        
        \textbf{Settings and Hyperparameters:} Settings element contain hyperparameters child element, and hyperparameters has all the common properties for the selected RL algorithms. The hyperparameters element contains alpha (the learning rate), gamma (the discount factor), epsilon (specifies the exploration rate), and total episodes (total number of episodes to train the agent).
        
        \textbf{QLearning, SARSA and DQN:} These elements extend the Reinforcement Learning Algorithm element, and therefore is associated with the "Settings" and "Hyperparameters" entities. The language metamodel is easily extensible and support adding more RL algorithms as the language matures, to cover more tests cases and broader RL problems.
        
        \textbf{Result:} The result element contains a result string property, and is used to display the results of running the chosen algorithm with the inserted settings on the provided RL problem environment through states, actions, rewards and terminal states properties, and display it in the RLML editor.
        
        % The following figures, Figure \ref{fig:QLearningSample} and Figure \ref{fig:SARSASample}, are abstract syntax tree sample models, based on the RLML metamodel. The elements property values are replaced with the values which represent the samples. The first sample model project is called \texttt{PathFindingQLearning} and the second one is called \texttt{PathFindingSARSA}. They are mostly similar, since they are referring to the same environment, the main difference is the algorithm chosen to solve the problem, QLearning and SARSA respectively. These samples illustrate the simplicity that the language is offering. The user of RLML only needs to provide environment and algorithm, and all the nitty-gritty details are hidden in the language and left for RLML to consider and calculate.
        
        % \begin{figure}[!tbh]
        %   \centering
        %   \includegraphics[width=13cm]{images/PathFindingQLearning.png}
        %   \caption{Path Finding QLearning AST Sample Model}
        %   \label{fig:QLearningSample}
          
        %   \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764
          
        %   \includegraphics[width=13cm]{PathFindingSARSA}
        %   \caption{Path Finding SARSA AST Sample Model}
        %   \label{fig:SARSASample}
        % \end{figure}
        
    \section {RLML Concrete Syntax}
        During RLML development, we had the goal of simplifying the complexity involved in reinforcement learning applications. Therefore, the proposed concrete syntax embodies this goal and concept and represents the abstract syntax as a simple configuration like properties file. RLML concrete syntax is of textual type, as shown in Listing \ref{lst:RLMLConcreteSyntax} that represents a sample RLML model.
        
        \begin{minipage}{\linewidth}
            \begin{lstlisting}[caption={RLML Concrete Syntax.},label={lst:RLMLConcreteSyntax}]
              RLML Project Name: SimpleExample 
             
              RL Environment: 
                States: [A,B] 
                Actions: [[1],[0]] 
                Rewards: [[0,0],[0,1]] 
                Terminal States : [A] 
               
              RL Agent: 
                RL Algorithm: Q-Learning 
                Hyperparameters: 
                  Alpha: 0.1 
                  Gamma: 0.9 
                  Epsilon: 0.9 
                  Total Episodes: 1000 
               
            \end{lstlisting}
        \end{minipage}
        
        The inspiration of the RLML concrete syntax comes from YAML representation, which is a human-readable data format used for data serialization. It is used for reading and writing data independent of a specific programming language.
        From the sample RLML model shown in Listing \ref{lst:RLMLConcreteSyntax}, we can see how RLML abstract syntax elements are mapped to key value pairs in the concrete syntax. The language is mainly expecting to receive the project's name, the environment element properties, which are the states, actions, rewards and terminal states, and the agent's RL algorithm type and the settings for that algorithm.

    \section {RLML Validation Constraints}
        To be able to verify that the RLML textual model, created by a user, is valid, all the property values must be in a valid format. The property values are considered valid when they are in a format that RLML can use them to implement the chosen RL algorithm. To ensure that the user is entering valid properties, we developed the following DSL validation constraints.
    
        \textbf{States Constraint:} States property value is expecting a string representation of all the possible states an agent can move within the current world or the environment of the current task. The value of the states property must be a comma-separated list of state strings, within square brackets.\newline
        Valid example: [A, B, C, D, E, F]
        
        \textbf{Actions Constraint:} The possible actions that the agent can take for each state of the states array. This value is also in string format and expects an array of arrays of indexes. The array of indexes contain the index values of the states that the agent can go to, starting from the given state. Each array is a comma-separated list within square brackets. The constraint validates the format of the provided string value and checks that the length of actions array element is equivalent to the length of the states array element. In the valid example below, we can see that there are six arrays of indexes to match the length of states example array.\newline
        States example: [A, B, C, D, E, F]\newline
        Valid Example: [[1,3], [0,2,4], [2], [0,4], [1,3,5], [2,4]]
        
        \textbf{Rewards Constraint:} Rewards property value is similar to actions property value, however the difference is instead of expecting an array of arrays of indexes, it expects an array of arrays of reward values the agent will receive when moving from the given state to other states in the environment. The RL algorithm will eventually learn to move towards the states that give maximum future rewards and ignore the ones that do not give rewards. Each array is a comma-separated list within square brackets. Similar to actions value validation, the rewards constraint validates the format of string and checks that the length of rewards array element, is equivalent to the length of the states array element. In the valid example below, there are also six arrays of six reward values to match the length of states example array.\newline
        States example: [A, B, C, D, E, F] \newline
        Valid Example: [[0,0,0,0,0,0], [0,0,100,0,0,0], [0,0,0,0,0,0], [0,0,0,0,0,0], [0,0,0,0,0,0], [0,0,100,0,0,0]]
        
        \textbf{Terminal States Constraint:} In the reinforcement learning domain, the terminal states is a subgroup of all the states that can end a training episode, either because it is the goal state or because it is a terminating state. Therefore, the terminal states array should provide a smaller string array of the states array. The terminal states constraint ensures the format of the string value provided is as comma-separated list within square brackets and checks that this array is a sub-array of the states example array.\newline
        States example: [A, B, C, D, E, F]\newline
        Valid Example: [C]

\chapter{Domain-Specific Modelling Environment}
    There are various available domain-specific modelling environments to chose from, when developing a DSL. To list a few, let's look at the following: Eclipse Papyrus \cite{Papyrus}, Xtext \cite{Xtext}, AToM3 \cite{AToM3}, MetaEdit+ \cite{MetaEdit+}, etc. We have chosen MPS JetBrains because it is well supported through tutorials, forum and support contacts. Additionally it is a commercial product, hence it is more stable and user-friendly than some of the available research prototypes.
    
    \label{chap:MPS}
    \section{MPS JetBrains}
        The proposed DSL for the reinforcement learning domain is developed using MPS (Meta Programming System)~\cite{MPS}, which is a modelling language workbench developed by JetBrains. Developing a DSL in MPS is performed by defining and maintaining the language's abstract syntax tree (AST). AST holds the data structure of the language that consists of nodes, AST-nodes, with a set of attributes (properties, children, and references) that describe the DSL in hand. The AST-nodes are defined by what is called as concepts in MPS.
        
        The most important feature of MPS is that it is not a traditional textual editor, it is instead a projectional editor \cite{MPS}, so what MPS does is that it projects the DSL's abstract syntax tree, and it always preserves the language as an AST whether in the disk or when editing, instead of traditional textual editing and parsing back to AST of the implemented language. Code in MPS is not text, but instead it is a projection of the abstract syntax tree. Projectional editing is not a new concept, however MPS put a great effort in making projectional editing as smooth and user-friendly as textual editing.
    
    \section{MPS Environment}
    \label{sec:MPS Environment}
        MPS \cite{MPS} is a language workbench which provides a tool or set of tools to support language definition, and it implements language-oriented programming. MPS is an integrated development environment (IDE) for DSL development, which promotes reusability and extensibility.
        
        The language definition in MPS consists of several aspects. Only one of these aspects is essential for language definition, which is the structure aspect and the rest is for additional features. These aspects describe different facets of a language. The available list of aspects in MPS are: structure, editor, actions, constraints, behavior, type system, intentions, plugin and data flow.
        
        We have employed structure, editor and constraints aspects in the RLML definition. The structure aspect, defines the nodes of the language AST, known by concepts in MPS. Editor describes how a language will be presented and edited in the editor and it enables the language designer to create a UI for editing their concepts, and constraints describe the constraints on the AST.
        
        \textbf{RLML structure:} the structure aspect contains the concepts that represent the RLML metamodel. Each concept consist of properties and children, reflecting the properties and the relationships in the RLML metamodel, shown in Fig. \ref{fig:RLMLMetamodel}.
        
        \textbf{RLML editor:} in MPS we can create an editor for each concept of the language structure (known as concept editor). The RLML editor aspect is configured to define RLML's concrete syntax as described and illustrated earlier in Listing \ref{lst:RLMLConcreteSyntax}. The concept editor for RLML root element contains "Run Program" and "Clear Result" buttons (see Fig. \ref{fig:RLMLModelMPS}). The first button (Run Program) gets RLML model generated Java class, runs the program in MPS environment, and displays the result in a designated text area in the editor. The second button (Clear Result) clears the result from the designated text area. These buttons help to easily get the generated Java code, run it and see the results in the proposed language editor.
        MPS tool is a projectional editor (as described earlier). Therefore, it offers out of the box features, such as automatic code completion that shows suggestions as the user types the DSL's syntax. Code completion feature helps the RLML user to see the list of available reinforcement learning algorithms (refer to \ref{fig:MPSCodeCompletion}) and chose a fit solution to the targeted RL problem.
        
        \begin{figure}[!tbh]
            \centering
            \includegraphics[width=13cm]{images/MPSCodeCompletion.png}
            \caption{RLML MPS Code Completion}
            \label{fig:MPSCodeCompletion}
        \end{figure}
        
        \textbf{RLML constraints:} the proposed DSL validation constraints are implemented using MPS concept constraints capability. For each defined structure concept, we can develop a concept constraint to validate it. RLML constraints aspect reflect the proposed language's constraints (explained in detail earlier), which are actions, rewards, states and terminal states constraints.
        
        \begin{figure}[!tbh]
            \centering
            \includegraphics[width=13cm]{images/MPSJetBrains/RLMLSampleModelMPS.png}
            \caption{RLML Model in MPS}
            \label{fig:RLMLModelMPS}
        \end{figure}
        
        MPS also provides different types of solution modules. Solution in MPS is the simplest kind of module. One of these solutions is called sandbox that facilitates implementing the developed language and holds the end user code. Fig.~\ref{fig:RLMLModelMPS} shows an example of an RLML model in MPS.
        
    \section{RLML Code Generation}
        This research work aims to abstract the complexity of reinforcement learning problem and algorithms and to generate a runnable code from the proposed DSL. Generators define possible transformations between a source modelling language and a target language, typically a general purpose language, like Java. Models get gradually transformed, where the nodes repeatedly get replaced with nodes at a lower level of abstraction until the desired level is reached. Code generation in MPS is applied by what is called the generator aspect, where we can define rules for translating our language concepts and map their properties into the lower level language concepts and properties.
        
        MPS supports model-to-model (M2M) transformations, as well as model-to-text (M2T) transformations, referred to as the TextGen aspect. For our proposed language, we implemented the M2M transformation to a Java output language, since it is currently the supported code generation language in MPS. The generator module of MPS, expects templates and mapping configurations, that define the transformation for generating code into the desired output language, which is Java in our case. The MPS generator module, contains the mapping configuration node, which is the minimal unit to generate a single transformation step. It is worth mentioning that in our implementation we only have one transformation step, from RLML to Java code. The mapping configuration groups the generator rules, which are the transformation template definitions. MPS has several types of transformation rules, like conditional root rule, root mapping rule, weaving rule, reduction rule, etc. We have used a root mapping rule and reduction rules for our code generation.
        
        \textbf{Root mapping rule:} RLML's generator module contains one root mapping rule for the RLML element, which is the root element of RLML. The rule specifies the template to transform the RLML element or RLML concept in MPS, into a valid Java class with fields and methods corresponding to those in RLML element's properties and children.

        \textbf{Reduction rule:} RLML's generator module contains two reduction rules; one for Q-Learning algorithm and one for SARSA algorithm. Supporting more reinforcement learning algorithms, simply means, extending the language to contain more RL algorithm concepts and their reduction rules/transformation rules. RLML structure aspect contains three model-free gradient-free reinforcement learning algorithms: Q-Learning, SARSA and DQN. Due to time constraints, mapping for Deep Q-Learning (DQN) algorithm has not been defined. The proposed language is easily extensible, hence support for DQN can be added by first adding a Java library with deep learning RL algorithms support to RLML MPS project, and next creating the reduction rule, using the added RL Java library, for DQN algorithm concept.
        
        \begin{itemize}
            \item Reduce Q-Learning rule: Defines the Java transformation template for Q-Learning algorithm calculation.
            \item Reduce SARSA rule: Defines the Java transformation template for SARSA algorithm calculation.
        \end{itemize}
        
         Based on these transformation rules. MPS can transform an RLML model to Java code. The generated Java class contains around two hundred lines of code, and this emphasizes the simplicity offered by RLML. The name of the class will be mapped to RLML element's project name property, and it contains a method called run() which implements the reinforcement learning algorithm calculations based on the reduction rules defined earlier. This part of the class, is the result of the reduce Q-Learning or SARSA transformation rule implementation. A demo video of the RLML modelling environment is available at \url{https://drive.google.com/drive/u/4/folders/1EfcsSdkBSkJyjPdlwhmGchnqZYYagpBA}.

\chapter{Validation}
\label{chap:validation}
    To validate the proposed reinforcement learning domain-specific language, we demonstrate it on three applications: path finding, simple game and frozen lake (refer to the RLML demo video \url{https://drive.google.com/drive/u/4/folders/1EfcsSdkBSkJyjPdlwhmGchnqZYYagpBA} for more details). These applications are well-known real world applications used in the reinforcement learning domain \cite{Ravichandiran2018}. We validated RLML with both Q-Learning and SARSA algorithms for each application. Integration of DQN is currently work in progress, hence the validation does not cover DQN examples.
    
    \section{Path Finding Application}
    Our first application of reinforcement learning problem is called path finding problem \cite{Verma2020}, which is a common application in machine learning domain that can be solved with different algorithms, including RL algorithms. In the path finding environment, the agent's goal is to learn the path to a target state, starting from a randomly selected state (see Fig. \ref{fig:PathFindingEnv}).
    
    There are in total six states in our application, represented by the alphabetical letters A to F (see Fig. \ref{fig:PathFindingEnv}). On each episode the agent will be placed in a random state, from there, it will take actions, and move to new states trying to reach the goal state, which is C for this application. Once the agent reaches the goal state, that episode will be considered complete. The agent will repeat the training episodes a given number of times, as configured in the RL algorithm. In an RLML model, this is set as the total episodes in the RL algorithm entity. At the end of the training, the agent will learn the best path to the goal state C, starting from the random initial state. The agent learns the path to the goal state by updating what is referred to as \emph{Q-Table} in reinforcement learning domain, and aims to calculate the optimal action value function that can be used to derive the optimal policy. 
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=6cm]{images/PathFinding/PathFindingEnv.png}
        \caption{Path Finding Environment}
        \label{fig:PathFindingEnv}
    \end{figure} 
    
   The textual model for the RL environment must conform to the RLML abstract and concrete syntax (described in Chap.~\ref{chap:MPS}). Simply put, the RL environment needs to be written in a format that RLML expects and can work with during code generation and and execution of the RL algorithm. Therefore, we write the path finding application environment as states, actions, rewards and terminal states arrays as shown in Listing \ref{lst:PathFindingEnvInput}.
    
    \begin{minipage}{\linewidth}
         \begin{lstlisting}[caption={The Input Values for Path Finding Environment.},label={lst:PathFindingEnvInput}]
            States: [A, B, C, D, E, F] 
            Actions: [[1,3], [0,2,4], [2], [0,4], [1,3,5], [2,4]]
            Rewards: [[0,0,0,0,0,0], [0,0,100,0,0,0], [0,0,0,0,0,0], [0,0,0,0,0,0], [0,0,0,0,0,0], [0,0,100,0,0,0]]
            Terminal States: [C]
        \end{lstlisting}
    \end{minipage}
    
    Next, the path finding application environment variables and reinforcement learning algorithm option needs to be inserted in the RLML path finding Q-Learning model, as shown in Fig. \ref{fig:PathFindingQLearningRLML}.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/PathFinding/PathFindingQLearningRLML.png}
        \caption{RLML Application 1: Path Finding Q-Learning}
        \label{fig:PathFindingQLearningRLML}
    \end{figure}

     The Java code automatically generated from the RLML model is presented in Fig.~\ref{fig:PathFindingQLearningJava}. At a high level, it is a Java class named according to the RLML root element name property, \texttt{PathFindingQLearning}. It contains a method to implement the chosen algorithm, a method to run it and a one to print the results.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/PathFinding/PathFindingQLearningJava.png}
        \caption{RLML Application 1: Path Finding Q-Learning - Generated Java Code}
        \label{fig:PathFindingQLearningJava}
    \end{figure} 
    
    RLML modelling environment contains the \texttt{Run Program} button (explained earlier in Sec.~\ref{sec:MPS Environment}). Once we click on the \texttt{Run Program} button, the environment is dynamically updated with the calculated results and we can see the result of running the Java program in the editor itself (see Fig. \ref{fig:PathFindingQLearningResult}). 
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/PathFinding/PathFindingQLearningResult.png}
        \caption{RLML Application 1: Path Finding Q-Learning - Result}
        \label{fig:PathFindingQLearningResult}
    \end{figure}
    
    The \emph{Q-Table} and policy are dynamically calculated and displayed. This can be viewed in the modelling environment in the \emph{Results} section. The policy which is derived from \emph{Q-Table} values, shows the preferred action the agent will make at each state. We can see in Table \ref{table:PathFindingQLearningQTablePolicy} (this table prints the result shown in Fig. \ref{fig:PathFindingQLearningResult} for clarity) that the agent learned to go to state B from state A, to state C from state B, and so on. Over all, the agent learned the shortest path to go to the target state, which is C.
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|l|} 
        \hline
        Q-Table Result: \\
        A:  0 90 0 72.9 0 0 \\ 
        B:  81 0 100 0 81 0 \\
        C:  0 0 0 0 0 0  \\
        D:  81 0 0 0 81 0 \\
        E:  0 90 0 72.9 0 90 \\ 
        F:  0 0 100 0 81 0 \\
        \\
        Policy: \\
        From A go to B \\
        From B go to C \\
        From C go to C \\
        From D go to A \\
        From E go to B \\
        From F go to C \\ [1ex]
        \hline
    \end{tabular}
    \caption{Q-Table and Policy Values for Path Finding Q-Learning} \label{table:PathFindingQLearningQTablePolicy}
    \end{table}
    
    So far the application was implementing the Q-Learning algorithm, however we can easily substitute the algorithm to SARSA algorithm instead. In Fig.~\ref{fig:PathFindingSARSARLML}, we illustrate how RLML model looks when using SARSA algorithm for the path finding application (Fig.~\ref{fig:PathFindingEnv}). The difference between Q-Learning implementation and SARSA implementation is minor at the RLML level. RLML only shows the algorithm type and hyperparameters necessary for the SARSA algorithm to run. However, it will handle the details of the algorithm calculations during code generation and based on that, it will produce the valid results. 
    
    The \emph{Q-Table} and policy for the path finding SARSA application (Fig. \ref{fig:PathFindingSARSAResult}) slightly differs from the results in the path finding Q-Learning application. In both cases, the agent successfully learns the path to the goal state (also presented in Table~\ref{table:PathFindingSARSAQTablePolicy} for clarity). 

    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=12cm]{images/PathFinding/PathFindingSARSA-RLML.png}
        \caption{RLML Application 1: Path Finding with SARSA}
        \label{fig:PathFindingSARSARLML}
    \end{figure}
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=12cm]{images/PathFinding/PathFindingSARSA-Java.png}
        \caption{RLML Application 1: Path Finding with SARSA  - Generated Java code}
        \label{fig:PathFindingSARSAJava}
    \end{figure}
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=12cm]{images/PathFinding/PathFindingSARSA-Result.png}
        \caption{RLML Application 1: Path Finding with SARSA - Result}
        \label{fig:PathFindingSARSAResult}
    \end{figure}
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|l|} 
        \hline
        Q-Table Result: \\
        A:  10.25 13.8 0 8.63 9.53 21.79 \\
        B:  7.81 23.32 100 7.52 9.92 14.01 \\
        C:  0 0 0 0 0 0 \\
        D:  10.23 8.14 0 10.82 13.01 6.4 \\
        E:  14.81 16.94 0 7.61 16.84 30.51 \\
        F:  6.56 59.87 100 4.37 5.44 41.63 \\
        \\
        Policy: \\
        From A go to B \\
        From B go to C \\
        From C go to C \\
        From D go to E \\
        From E go to F \\
        From F go to C\\ [1ex]
        \hline
    \end{tabular}
    \caption{Q-Table and Policy Values for Path Finding with SARSA} \label{table:PathFindingSARSAQTablePolicy}
    \end{table}
    
    \section{Simple Game Application}
    
    The second application we used to demonstrate the capabilities of RLML on is a simple game example \cite{simpleGame} (see Fig. \ref{fig:SimpleGame}). This application has a similar environment to the path finding application. The agent here is represented by the player who is targeting a goal state C. In addition to reaching the goal state C, the player needs to avoid danger states which represent some danger concept, for example a fire, on the player in the game environment. The dangerous states reward the player with negative 10, by this negative 10 value, the player will learn to avoid these states, and will learn the path to the target state avoiding the dangerous states.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=6cm]{images/SimpleGame/SimpleGame.jpg}
        \caption{Simple Game Environment}
        \label{fig:SimpleGame}
    \end{figure}
    
    Just like we did in the path finding application, we need to convert the physical environment of this simple game application to values RLML can work with. The input values for the simple game RL problem environment are shown in Listing \ref{lst:SimpleGameEnvInput}.
    
    \begin{minipage}{\linewidth}
        \begin{lstlisting}[caption={The Input Values for Simple Game Environment.},label={lst:SimpleGameEnvInput}]
            States: [A, B, C, D, E, F, G, H, I]
            Actions: [[1,3], [0,2,4], [2], [0,4,6], [1,3,5,7], [2,4,8], [3,7], [4,6,8], [5,7]]
            Rewards: [[0,0,0,0,0,0,0,0,0], [0,0,5,0,-10,0,0,0,0], [0,0,0,0,0,-10,0,0,0], [0,0,0,0,-10,0,0,0,0], [0,0,0,0,0,-10,0,0,0], [0,0,5,0,-10,0,0,0,0], [0,0,0,0,0,0,0,0,0], [0,0,0,0,-10,0,0,0,0], [0,0,0,0,0,-10,0,0,0]]
            Terminal States: [C]
        \end{lstlisting} 
    \end{minipage}
    
    In Fig. \ref{fig:SimpleGameRLML}, we present the simple game Q-Learning RLML model by inserting the environment values and choosing Q-Learning algorithm and providing Q-Learning hyperparameters.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/SimpleGame/SimpleGameRLML.png}
        \caption{RLML Application 2: Simple Game Q-Learning}
        \label{fig:SimpleGameRLML}
    \end{figure} 
    
    As we saw earlier in the path finding application, we can get the generated Java code as a class named \texttt{SimpleGameQLearning}, run the program and get the results with the click of a button "Run Program" within the RLML editor, as presented in Fig. \ref{fig:SimpleGameResult}.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/SimpleGame/SimpleGameResult.png}
        \caption{RLML Application 2: Simple Game Q-Learning - Result}
        \label{fig:SimpleGameResult}
    \end{figure} 
    
    If we look at the calculated \emph{Q-Table} and policy in Table \ref{table:simpleGameQTablePolicy} (this table
    shows the result from Fig.~\ref{fig:SimpleGameResult} for clarity), we can notice that the player learned the path to the goal state C, and also learned to avoid the danger states E and F, because in the \emph{Q-Table} values, states F and E are associated with negative values. Following this policy, the agent will avoid the states with negative values. In the printed policy path, we can see that the player never takes E or F states options. For example, from H it goes to G, and from I it goes to H, and not F. Even though there are fewer steps to reach to the goal state C through the danger states, the player will not take that path, because the player or the agent has learned to avoid the path with danger states, since it receives negative rewards.

    \begin{table}[h!]
    \centering
    \begin{tabular}{|l|} 
        \hline
        Q-Table Result: \\
        A:  0 4.5 0 3.64 0 0 0 0 0 \\
        B:  4.05 0 5 0 -5.95 0 0 0 0 \\
        C:  0 0 0 0 0 0 0 0 0 \\
        D:  4.05 0 0 0 -5.95 0 3.28 0 0 \\
        E:  0 4.5 0 3.64 0 -5.5 0 2.95 0 \\
        F:  0 0 5 0 -5.95 0 0 0 2.66 \\
        G:  0 0 0 3.64 0 0 0 2.95 0 \\
        H:  0 0 0 0 -5.95 0 3.28 0 2.66 \\
        I:  0 0 0 0 0 -5.5 0 2.95 0 \\
        \\
        Policy: \\
        From A go to B \\
        From B go to C \\
        From C go to C \\
        From D go to A \\
        From E go to B \\
        From F go to C \\
        From G go to D \\
        From H go to G \\
        From I go to H \\ [1ex]
        \hline
    \end{tabular}
    \caption{Q-Table and Policy Values for Simple Game with Q-Learning.} \label{table:simpleGameQTablePolicy}
    \end{table}
    
    Switching the algorithm to SARSA instead of Q-Learning is straightforward in the RLML environment by selecting SARSA as the RL algorithm option. The simple game application with SARSA model is shown in Fig~\ref{fig:SimpleGameSARSARLML}. The results of running simple game with SARSA is shown in Table~\ref{chap:RLML}.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=10cm]{images/SimpleGame/SimpleGameSARSARLML.png}
        \caption{RLML Application 2: Simple Game with SARSA}
        \label{fig:SimpleGameSARSARLML}
    \end{figure} 
    
    \begin{table}[h!]
    \centering
    \begin{tabular}{|l|} 
        \hline
        Q-Table Result: \\
        A:  -0 -0.02 0 -0.03 -0 -0.02 -0 -0.03 -0 \\
        B:  -0 0.04 5 -0.26 -10.01 -0.17 -0 -0.37 -0.02 \\ 
        C:  0 0 0 0 0 0 0 0 0 \\
        D:  -0 -0.2 0 -0.44 -10.01 -0.35 -0 -0.18 -0.01 \\
        E:  -0 -0.02 0 -0.03 -0.14 -10.03 -0 -0.03 -0.25 \\
        F:  -0 -0.18 5 -0.2 -10.02 -0.42 -0 -0.4 -0.04 \\
        G:  -0 -0.03 0 -0.04 -0 -0.03 -0 -0.03 -0 \\
        H:  -0 -0.37 0 -0.29 -10.02 -0.35 -0 -0.4 -0.04 \\ 
        I:  -0 -0.02 0 -0.03 -0.31 -10.03 -0 -0.04 -0.52 \\
        \\
        Policy: \\
        From A go to A \\
        From B go to C \\
        From C go to C \\
        From D go to D \\
        From E go to E \\
        From F go to C \\
        From G go to G \\
        From H go to H \\
        From I go to I \\ [1ex]
        \hline
    \end{tabular}
    \caption{Q-Table and Policy Values for Simple Game with SARSA.} \label{table:simpleGameQTablePolicySARSA}
    \end{table}
    
    \section{Application 3: Frozen Lake Application}
    
    The third application we have is called frozen lake (refer to Fig. \ref{fig:FrozenLakeEnv}). This is a popular application in reinforcement learning problems \cite{Ravichandiran2018}. The agent here will learn to navigate in the frozen lake environment avoiding what is called holes in the lake, because whenever the agent takes a step to a hole state, that will terminate the training episode, without reaching to the goal state, meaning the agent can not make any further actions from the hole states. The RLML model for frozen lake application is presented in Fig.~\ref{fig:FrozenLakeRLML}.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=6cm]{images/FrozenLake/FrozenLakeEnv.png}
        \caption{Frozen Lake Environment}
        \label{fig:FrozenLakeEnv}
    \end{figure}
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/FrozenLake/FrozenLakeRLML.png}
        \caption{RLML Application 3: Frozen Lake Q-Learning}
        \label{fig:FrozenLakeRLML}
    \end{figure}
    
    The result for frozen lake RL problem is shown in Table[\ref{table:FrozenLakeResult}]. These results are populated through the RLML frozen lake Q-Learning model and printed in the result text area (see Fig. \ref{fig:FrozenLakeResult}). Studying the \emph{Q-Table} and policy values we notice that the agent successfully learned to navigate the frozen lake environment and avoided any hole state. In the policy table, non of the states take an action to go to state H, except when the initial state starts from H, which will end the navigation and in turn the training episode.
    
    \begin{minipage}{\linewidth}
        \begin{lstlisting}[caption={The Input Values for Frozen Lake Environment},label={lst:label1}]
            States: [S, F, F, F, F, H, F, H, F, F, F, H, H, F, F, G]
            Actions: [[1,4], [0,2,5], [1,3,6], [2,7], [0,5,8], [5], [2,5,7,10], [7], [4,9,12], [5,8,10,13], [6,9,11,14], [11], [12], [9,12,14], [10,13,15], [15]]
            Rewards: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
            Terminal States: [G,H]
        \end{lstlisting}
    \end{minipage}

    \begin{table}[h!]
    \centering
    \begin{tabular}{|l|} 
        \hline
            Q-Table Result: \\
            S:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0.01 0 0 0.01 0 0 \\ 
            H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0.01 0.01 0 0 0.01 0.01 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0.03 0 0 0.02 0.07 0 \\
            H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            F:  0 0 0 0 0 0 0 0 0 0 0.02 0 0 0.03 0.14 0 \\
            F:  0 0 0 0 0 0 0 0 0 0.01 0.02 0 0 0.02 0.31 1 \\
            G:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
            \\
            Policy: \\
            From S go to F \\
            From F go to F \\
            From F go to F \\
            From F go to F \\
            From F go to F \\
            From H go to H \\
            From F go to F \\
            From H go to H \\
            From F go to F \\
            From F go to F \\
            From F go to F \\
            From H go to H \\
            From H go to H \\
            From F go to F \\
            From F go to G \\
            From G go to G \\ [1ex]
        \hline
    \end{tabular}
    \caption{Q-Table and Policy Values for Frozen Lake with Q-Learning} \label{table:FrozenLakeResult}
    \end{table}
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/FrozenLake/FrozenLakeResult.png}
        \caption{RLML Application 3: Frozen Lake with Q-Learning - Result}
        \label{fig:FrozenLakeResult}
    \end{figure}

    Similar to the path finding application and simple game applications, we demonstrate the frozen lake application with SARSA algorithm (see Fig.~\ref{fig:FrozenLakeSARSARLML}). The computed result is shown in Table~\ref{table:FrozenLakeSARSAResult}.
    
    \begin{figure}[!tbh]
        \centering
        \includegraphics[width=13cm]{images/FrozenLake/FrozenLakeSARSARLML.png}
        \caption{RLML Application 3: Frozen Lake with SARSA}
        \label{fig:FrozenLakeSARSARLML}
    \end{figure}

    \begin{table}[h!]
    \centering
    \begin{tabular}{|l|} 
        \hline
        Q-Table Result: \\
        S:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0.01 0 0 0 0 0 \\ 
        H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  \\
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0.01 0 0 0.01 0 0 \\ 
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0 \\
        H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        H:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0 \\
        F:  0 0 0 0 0 0 0 0 0 0 0.01 0 0 0.01 0 1 \\ 
        G:  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\
        \\
        Policy: \\
        From S go to F \\
        From F go to F \\
        From F go to F \\
        From F go to F \\
        From F go to F \\
        From H go to H \\
        From F go to F \\
        From H go to H \\
        From F go to F \\
        From F go to F \\
        From F go to F \\
        From H go to H \\
        From H go to H \\
        From F go to F \\
        From F go to G \\
        From G go to G \\
        \hline
    \end{tabular}
    \caption{Q-Table and Policy Values for Frozen Lake with SARSA} \label{table:FrozenLakeSARSAResult}
    \end{table}

\chapter{Related Work}
\label{chap:related work}
    To the best of our knowledge, there is limited work available in this area. Domain-specific modelling languages for the artificial intelligence domain and more specifically, machine learning domain, are recent contributions, driven by the need to make machine learning algorithms easier to understand, more attractive to apply and faster to test and compare different solutions to one another. 
    
    Bucchiarone et al \cite{Bucchiarone2020} analyses the state of research, state of practice, and state of the art in model-driven engineering (MDE). It summarizes these challenges and presents a set of grand challenges to lead the research initiatives in MDE community. One of the challenges this paper describes is the technical challenges in MDE, and the need to improve MDE solutions for complex systems. It suggests exploiting AI techniques to make MDE more powerful, or using MDE techniques to help in the improvement of AI and machine learning applications. It states that machine learning practice would be easier if the learning curve for the needed skills was more convenient. Our research is motivated by this challenge and is a contribution towards the suggested improvement. With our DSL we tried to explore how model-driven software engineering can help machine learning implementations, through abstractions, and from these abstractions, enable automatic code generation, which is one of the suggested research work in the grand challenges in model-driven engineering paper.
    
    Our work takes inspiration from the Classification Algorithm Framework (CAF) \cite{meacham2020}. The primary difference between CAF and RLML is that CAF is developed for machine learning classification algorithms, and RLML is developed for reinforcement learning algorithms. CAF expects inputs to implement classification algorithms on, and RLML expects inputs to implement reinforcement learning algorithm. They both support code generation in Java language, and have similar configuration-like interface where it interacts with the user.
    
    ThingML \cite{Harrand2016} is also a modelling language for the machine learning domain. It includes a tool designed for supporting a highly customizable multi-platform code generation framework. The purpose of ThingML is to allow the modelling of various software components and to automatically generate complete code modules ready to be deployed. ThingML code generation framework defines a family of compilers able to transform a ThingML model into fully operational code in various languages. Each compiler is composed of a set of code generators, each of them responsible for the compilation of a specific subset of ThingML. In our case RLML code generation does not depend on compilers, and it is executed through MPS codegen aspect which relies on transformation mapping rules implementing model-to-model transformations.
    
    DeepDSL \cite{Zhao2017} is a domain-specific language embedded in Scala, for developing deep learning applications. It was developed to address the limitations which exist in other deep learning tools, and provide compiler-based optimizations for the deep learning models to run with less memory usage and/or in shorter time. DeepDSL allows users to define deep learning networks as tensor functions, and similar to ThingML, it has its own compiler that produces DeepDSL Java program. Therefore, it is quite different than RLML, which is developed through MPS modelling tool and for the RL domain.
    
    There is also a project called AutoML \cite{AutoML}. The AutoML \cite{AutoML} project targets machine learning automation. In traditional machine learning, models are developed by hand, whereas AutoML offers automation in the process of making machine learning models from handling dataset to deploying a practical machine learning model. Both RLML and AutoML have the same goal of simplifying the process of handling the complicated steps involved in machine learning, however, they significantly differ in implementation. AutoML does not use MDE methods and tools and it focuses on various machine learning algorithms and works towards providing automation in the steps, so there is minimal human experts involvement. Whereas, RLML offers a DSML for the reinforcement learning domain in order to make RL more user-friendly.
    
    In addition to the above, there are existing Python libraries and frameworks that support reinforcement learning algorithms (e.g., RL-coach \cite{RL-coach}) or Tensorflow libraries (e.g., Tensorforce \cite{tensorflow2015-whitepaper}, TRFL \cite{TRFL} and TF Agents\cite{TFAgents}). The main difference is that these libraries are written in the general-purpose language, Python, and to develop reinforcement learning applications through them, requires the user to be involved in all the details of the algorithm implementation and calculations, which is what we tried to minimize with the abstraction provided in RLML.
    
    The presented related work overview, varies in technologies and approaches. Our proposed approach is unique and different than the rest of the reviewed work, because we use model-driven engineering technologies to abstract reinforcement learning domain. Therefore, we offer the benefits of model-driven engineering technologies to reinforcement learning algorithm users.

\chapter{Conclusions}
\label{chap:conclusions}
    With this research work, we proposed a domain-specific language (DSL) for the field of reinforcement learning, which we called RLML. With our DSL, we contributed in the area of MDE intelligence, more specifically, in the area of model-driven engineering for artificial intelligence \cite{MDEAI}. We saw how by cross collaboration between MDE and AI, artificial intelligence can benefit from model-driven engineering technologies. With the use of the language workbench MPS, we built a domain-specific modelling environment (DSME) for RLML, which offers model editing, syntax checking, constraints checking and validation, as well as Java code generation features. Our proposed language is developed to be easily extensible to support wide range of RL algorithms. However, currently we targeted model-free, gradient-free type of reinforcement learning algorithms.
    
    Through our reinforcement learning applications, we validated our proposed language. With our validation, we showcased how RLML achieved the abstraction needed in reinforcement learning applications, by providing a configuration like model where it is only expecting input values of the reinforcement learning problem environment and a choice of RL algorithm. From that point, the RLML modelling environment can generate executable code, run it and display the results. In the Chap.~\ref{chap:related work}, we went though existing work that are related to our research work. However, we realized that there exists minimal work in this area yet. This work is a starting point towards developing an environment for supporting various types of RL technologies. We have set the pillars and expect more work to be built upon this. 

    \section{Challenges}
    \label{section:challenges}
     During development of RLML, we faced different types of obstacles. The first one was regarding the domain we chose. Reinforcement learning is a rich domain, and to be able to develop a more complete DSL for the reinforcement learning domain, we need reinforcement learning experts input and guidance, so we can build our cases from their everyday experiences. The second challenge was regarding the tool we used for developing the proposed language, which is MPS. MPS is a well-supported tool with tutorials and forum. However, there is a learning curve to get familiar with the tool and to find helpful references when developing some advanced features. Another challenge we faced was due to the target code our language generates, Java. Most machine learning libraries are widely available as Python libraries and not as Java libraries. Python is the defacto language for machine learning applications. It was even more challenging to find Java libraries to support Reinforcement learning algorithms, as it is still less common and known compared to other forms of machine learning approaches. Even when we found some libraries, they were not fully supported. This challenge is due to the code generation support available in MPS, and could be avoided by using other language workbenches. There are some efforts to support Python code generation in MPS in a form of model-to-text code generation. However, it is still in the initial phases. As a first approach, we wanted to explore the well-supported Java in MPS, and we left Python code generation for further attempts.
     Also there was a challenge due to the fact that RL problems do not have fixed input format from actions, rewards and states perspective. Therefore, it was difficult to come up with a format for those inputs. More validation and mapping is needed for broader RL problem coverage and code generation.

    \section{Future Work}
    There are several extensions that can be made to contribute to the future of this project. To begin with, we can extend the language to cover more RL approaches and algorithms. As we saw in Sec.~\ref{subsection:Reinforcement Learning Algorithms} there are many types of RL algorithms. RLML supports SARSA, Q-Learning and DQN as available reinforcement learning algorithms, which fall under model-free gradient-free RL algorithms, and it supports code generation for SARSA and Q-Learning algorithms. Mapping rules for Deep Q-Learning algorithm has not been completed due to the challenges explained in Sec.~\ref{section:challenges} and time constraints. To have a more complete DSL we need to add more RL algorithm types. We can work on adding more value-based algorithms, or even work towards adding different type of algorithms like model-based and policy-based, etc. Extending the language to support more algorithms should not require major changes in the metamodel of the DSL, since It is developed as a generic language to cover various RL algorithms.
     
    Another possible path of future work is to add support for generating Python code from the RLML models. We have chosen Java, because MPS fully supports code generation for Java language. It also supports running the generated Java code in the MPS tool, which facilitates the process of testing and presenting the results. However, due to the explained in Sec.~\ref{section:challenges} specifically regarding the limited support for machine learning or reinforcement learning algorithms in Java, it will be good to additionally produce code in Python code.
    
    Finally, another important aspect to consider as future work is scalability. The scale of the data that is used to validate RLML is not large. We can implement RLML on real life applications with large input data; meaning large reinforcement learning environments from states, actions and rewards. Taking this path will test the language against large scale application from real life scenarios. We can also apply RLML on business case studies and get feedback from different level of expertise in reinforcement learning users.

\end{ryersonSGSThesis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                     Back matter                            %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \appendix
% \appendixpage       
% \addappheadtotoc

\clearpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}

% \clearpage
% \printglossary[type=\acronymtype]
% \printglossary[type=main]

% \clearpage
% \addcontentsline{toc}{chapter}{\indexname}
% \printindex

\end{document}
